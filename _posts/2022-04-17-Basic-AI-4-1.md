---
title:  "인공기능의 기초 4-1 강화 학습 소개"
layout: single
date: 2022-04-17T10:00:00+09:00
last_modified_at: 2022-04-17
category: Basic-AI
use_math: true
---
  
# 강화 학습 Reinforcement Learning
에이전트를 학습 시키는 과정중 하나인데, 어떤 행동을 했을 때 잘한 행동이면 보상을 잘하지 못한 행동이면 벌을 주는 식으로 학습하는 과정이다. 어떻게 하라고 직접 가르치는 것이 아닌 스스로 할 수 있게끔 하고 그 결과에 대해 보상과 벌을 결정하는 방법이다. 직접 어떻게 하라고 가르치는 것보다 효율이 떨어지지만, 게임처럼 몇 가지 룰을 제외하고서 아주 다양한 상황에서 임기응변을 해야하는 경우가 많을 때는 스스로 수많은 게임을 통해서 학습하는 것이 더 나을 것이다.  
  
**보상 함수**를 통해서 학습을 하게 되고, 자세한 내용은 agent가 직접 배워야한다. 그리고 보상과 벌이 지연될 수 있고(바둑에서 한 번 수를 둘 때마다 평가하는 것이 아닌 끝까지 해봐야 지고 이기고를 알고 이에 대한 보상과 벌을 줄 수 있으므로), **타이밍**이 중요하다. agent가 점점 학습해나가기 때문에 non i.i.d(independent identical distribute) data, 다 독립 사건이 아니고 agent의 행동이 다음에 받을 data에 영향을 미친다. 학습을 시키는 선생님 입장에서는 시간을 아낄 수 있지만 배우는 agent 입장에서는 **다양한 Trial&Error**를 통해서 스스로 배워야한다.   
  
![basic RL Model](/assets/img/2022-04-17-Basic-AI-4-1/1.png){: .center}  
  
강화 학습의 목적은 agent가 최대의 보상을 얻을 수 있는 control policy를 찾는 것이다.  
$S_t$ 상태에서 취할 수 있는 action $A_t$를 하고 나서 상태가 $S_{t+1}$로 바뀌고 직전 행동에 대한 보상 $R_{t+1}$이 얼마나 주어지는가를 반복한다. 이 과정을 반복함으로써 내가 어떤 상황에서 어떤 action을 취해야 할지 reward를 관측하여 스스로 배우도록 하는 것이다. 결국 reward라는 scaler 하나의 값을 통해서 학습을 하기 때문에 이를 어떻게 정의하고 주는지가 굉장히 중요하다.  
  
## RL Agent의 주요 요소 
다 가지는 경우도 일부만 가지는 경우도 있다.  
  
Policy: agent's behavior function 에이전트가 어떤 상황에서 어떤 action을 선택할지 정해주는 함수이다.  
Value function: how good is each state and/or action 내가 어떤 state에서 다른 state로 갈 때 그 state가 얼마나 좋은지 나타내는 함수이다.  
Model: agent's representation of the environment 에이전트가 환경을 어떻게 표현하고 있는지를 통칭한다.  
  
### Policy
state에서 action에 대한 함수이다. 크게 Deterministic policy와 Stochastic policy가 있다.  
Deterministic policy는 어떤 상태에서 어떤 액션을 취할지 정해놓은 것이고, Stochastic policy는 확률분포로 주어지는 것이다. 따라서 어떤 액션을 취할 확률이 높긴 하지만 항상 그 액션을 취하는 것은 아니다.  
  
![policy](/assets/img/2022-04-17-Basic-AI-4-1/2.png){: .center}  

Policy의 notation으로는 $\pi$를 사용한다.  
  
### Value Function
가치 함수. 미래의 보상에 대한 예측이다.  
  
![Value Function](/assets/img/2022-04-17-Basic-AI-4-1/3.png){: .center}  
  
$V$라는 notation을 주로 쓰고 state에 대한 함수이다. ($Q$로 쓰는 경우도 있는데 이는 state와 action에 대한 함수)  
$\pi$를 아래첨자에 써주는 이유는 policy가 바뀌면 Value Function의 값도 바뀌기 때문이다. 따라서 $\pi$라는 policy를 따라갈 때 그 state의 Value Function이다.  
  
$S$라는 state에서 time step $t$에 따라서 계속 action을 취하고 state가 변하는 걸 반복할 텐데 이때의 미래에 받을 수 있는 Reward의 기댓값의 합이다. 하지만 먼 미래로 갈수록 $\gamma$라는 인자를 사용해서 discount를 해주기 때문에 $\gamma$값은 보통 0과 1 사이의 값이다.  
  
### Model
환경에 대한 representation, 크게 $P$와 $R$ 두 가지 구성요소로 이루어져 있다.  
  
![Model](/assets/img/2022-04-17-Basic-AI-4-1/4.png){: .center}  
  
$P$는 내가 $S$라는 state에 있을 때 $a$라는 action을 취했을 때 그 다음 state를 $S^\prime$으로 갈 확률분포이다.  
$R$은 $S$라는 state에서 $a$라는 action을 취했을 때 얻을 수 있는 reward에 대한 확률분포이다.  
따라서 강화학습 모델이라는 건 $P$와 $R$을 지칭한다고 볼 수 있다.  
  
  
### Maze Example
![Maze](/assets/img/2022-04-17-Basic-AI-4-1/5.png){: .center}  
  
rewards를 time-step마다 -1로 주면 쓸데없는 행동을 하면 할수록 -1을 더 많이 받으니 이 reward 함수를 활용하면 최단거리로 goal까지 갈 수 있다.  
action은 동서남북으로 가는 것이고, state는 agent의 location이다.  
  
![policy and value](/assets/img/2022-04-17-Basic-AI-4-1/6.png){: .center}  
  
많은 경우에 policy와 value function 사이에 밀접한 관계가 있고 하나만 알고 있어도 다른 하나를 유도할 수 있다.  
  
![model](/assets/img/2022-04-17-Basic-AI-4-1/7.png){: .center}  
  
흰색으로 표현되어 있는 게 $P$, 내재적으로 transition model을 정의하고 있는 것이고, 갈 수 있는지 없는지 확률적으로 정의한 것이다. -1을 reward function으로 움직일 때마다 -1이라는 벌을 받기 때문에 -1로 채워져 있는 것이다.  
  
## Exploration and Exploitation
'이런 action을 취했을 때 reward가 더 많았으니까 그 액션을 취해야지'하는 Exploitation과 일종의 random action을 취해서 '내가 이런 상황에서 어떤 action을 취하면 이런 reward를 받는구나'하는 경험을 쌓는 Exploration도 필요하다(고착화 방지). 우리가 식당을 갈 때도 가장 맛있는 곳을 물론 자주 가지만 새로운 식당을 시도해보는 경우처럼 말이다.  

## Prediction and Control
Prediction은 예측으로 어떤 policy $\pi$가 주어졌을 때 Value Function $V_\pi(S)$를 구하는 작업이다.  
Control은 제어로 Reinforcement Learning에서 최적의 policy, reward를 최대로 할 수 있는 policy를 찾는 과정이다.  

### example of Prediction and Control
![eopac1](/assets/img/2022-04-17-Basic-AI-4-1/8.png){: .center}  
  
위 예제에서는 agent가 어디에 있든지 다른 위치로 이동할 확률이 같은(동서남북 모두 갈 수 있다면 각 0.25) uniform random policy를 가지고 있을 때의 value function을 보여준다. 이 값을 구하는 건 다음 장에서 알아보자.  
  
![eopac2](/assets/img/2022-04-17-Basic-AI-4-1/9.png){: .center}  
  
$V_\*$은 최적의 value function을 의미하고, $\pi_\*$는 reward를 가장 최대화하는 policy를 나타낸다. 이 optimal policy를 구하는 방법도 다음 장에서 알아보자.  
  

**KMOOC에서 김건희 교수님의 인공기능의 기초 강의를 보실 수 있습니다.**